<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[06动态代理是基于什么原理？[转]]]></title>
    <url>%2F2020%2F03%2F08%2FJava%E6%A0%B8%E5%BF%8336%E8%AE%B206%2F</url>
    <content type="text"><![CDATA[编程语言通常有各种不同的分类角度，动态类型和静态类型就是其中一种分类角度，简单区分就是语言类型信息是在运行时检查，还是编译期检查。与其近似的还有一个对比，就是所谓强类型和弱类型，就是不同类型变量赋值时，是否需要显式地（强制）进行类型转换。那么，如何分类Java语言呢？通常认为，Java是静态的强类型语言，但是因为提供了类似反射等机制，也具备了部分动态类型语言的能力。言归正传，今天我要问你的问题是，谈谈Java反射机制，动态代理是基于什么原理？ 典型回答反射机制是Java语言提供的一种基础功能，赋予程序在运行时自省（introspect，官方用语）的能力。通过反射我们可以直接操作类或者对象，比如获取某个对象的类定义，获取类声明的属性和方法，调用方法或者构造对象，甚至可以运行时修改类定义。动态代理是一种方便运行时动态构建代理、动态处理代理方法调用的机制，很多场景都是利用类似机制做到的，比如用来包装RPC调用、面向切面的编程（AOP）。实现动态代理的方式很多，比如JDK自身提供的动态代理，就是主要利用了上面提到的反射机制。还有其他的实现方式，比如利用传说中更高性能的字节码操作机制，类似ASM、cglib（基于ASM）、Javassist等。 考点分析这个题目给我的第一印象是稍微有点诱导的嫌疑，可能会下意识地以为动态代理就是利用反射机制实现的，这么说也不算错但稍微有些不全面。功能才是目的，实现的方法有很多。总的来说，这道题目考察的是Java语言的另外一种基础机制： 反射，它就像是一种魔法，引入运行时自省能力，赋予了Java语言令人意外的活力，通过运行时操作元数据或对象，Java可以灵活地操作运行时才能确定的信息。而动态代理，则是延伸出来的一种广泛应用于产品开发中的技术，很多繁琐的重复编程，都可以被动态代理机制优雅地解决。从考察知识点的角度，这道题涉及的知识点比较庞杂，所以面试官能够扩展或者深挖的内容非常多，比如： 考察你对反射机制的了解和掌握程度。 动态代理解决了什么问题，在你业务系统中的应用场景是什么？ JDK动态代理在设计和实现上与cglib等方式有什么不同，进而如何取舍？ 这些考点似乎不是短短一篇文章能够囊括的，我会在知识扩展部分尽量梳理一下。 知识扩展1.反射机制及其演进对于Java语言的反射机制本身，如果你去看一下java.lang或java.lang.refect包下的相关抽象，就会有一个很直观的印象了。Class、Field、Method、Constructor等，这些完全就是我们去操作类和对象的元数据对应。反射各种典型用例的编程，相信有太多文章或书籍进行过详细的介绍，我就不再赘述了，至少你需要掌握基本场景编程，这里是官方提供的参考文档：https://docs.oracle.com/javase/tutorial/refect/index.html 。 关于反射，有一点我需要特意提一下，就是反射提供的AccessibleObject.setAccessible(boolean fag)。它的子类也大都重写了这个方法，这里的所谓accessible可以理解成修饰成员的public、protected、private，这意味着我们可以在运行时修改成员访问限制！ setAccessible的应用场景非常普遍，遍布我们的日常开发、测试、依赖注入等各种框架中。比如，在O/R Mapping框架中，我们为一个Java实体对象，运行时自动生成setter、getter的逻辑，这是加载或者持久化数据非常必要的，框架通常可以利用反射做这个事情，而不需要开发者手动写类似的重复代码。 另一个典型场景就是绕过API访问控制。我们日常开发时可能被迫要调用内部API去做些事情，比如，自定义的高性能NIO框架需要显式地释放DirectBuffer，使用反射绕开限制是一种常见办法。 但是，在Java 9以后，这个方法的使用可能会存在一些争议，因为Jigsaw项目新增的模块化系统，出于强封装性的考虑，对反射访问进行了限制。Jigsaw引入了所谓Open的概念，只有当被反射操作的模块和指定的包对反射调用者模块Open，才能使用setAccessible；否则，被认为是不合法（illegal）操作。如果我们的实体类是定义在模块里面，我们需要在模块描述符中明确声明： module MyEntities { // Open for refection opens com.mycorp to java.persisence; } 因为反射机制使用广泛，根据社区讨论，目前，Java 9仍然保留了兼容Java 8的行为，但是很有可能在未来版本，完全启用前面提到的针对setAccessible的限制，即只有当被反射操作的模块和指定的包对反射调用者模块Open，才能使用setAccessible，我们可以使用下面参数显式设置。 --illegal-access={ permit | warn | deny } 2.动态代理前面的问题问到了动态代理，我们一起看看，它到底是解决什么问题？首先，它是一个代理机制。如果熟悉设计模式中的代理模式，我们会知道，代理可以看作是对调用目标的一个包装，这样我们对目标代码的调用不是直接发生的，而是通过代理完成。其实很多动态代理场景，我认为也可以看作是装饰器（Decorator）模式的应用，我会在后面的专栏设计模式主题予以补充。通过代理可以让调用者与实现者之间解耦。比如进行RPC调用，框架内部的寻址、序列化、反序列化等，对于调用者往往是没有太大意义的，通过代理，可以提供更加友善的界面。代理的发展经历了静态到动态的过程，源于静态代理引入的额外工作。类似早期的RMI之类古董技术，还需要rmic之类工具生成静态stub等各种文件，增加了很多繁琐的准备工作，而这又和我们的业务逻辑没有关系。利用动态代理机制，相应的stub等类，可以在运行时生成，对应的调用操作也是动态完成，极大地提高了我们的生产力。改进后的RMI已经不再需要手动去准备这些了，虽然它仍然是相对古老落后的技术，未来也许会逐步被移除。这么说可能不够直观，我们可以看JDK动态代理的一个简单例子。下面只是加了一句print，在生产系统中，我们可以轻松扩展类似逻辑进行诊断、限流等。 public class MyDynamicProxy { public satic void main (String[] args) { HelloImpl hello = new HelloImpl(); MyInvocationHandler handler = new MyInvocationHandler(hello); // 构造代码实例 Hello proxyHello = (Hello) Proxy.newProxyInsance(HelloImpl.class.getClassLoader(), HelloImpl.class.getInterfaces(), handler); // 调用代理方法 proxyHello.sayHello(); } } interface Hello { void sayHello(); } class HelloImpl implements Hello { @Override public void sayHello() { Sysem.out.println(&quot;Hello World&quot;); } } class MyInvocationHandler implements InvocationHandler { private Object target; public MyInvocationHandler(Object target) { this.target = target; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { Sysem.out.println(&quot;Invoking sayHello&quot;); Object result = method.invoke(target, args); return result; } } 上面的JDK Proxy例子，非常简单地实现了动态代理的构建和代理操作。 首先，实现对应的InvocationHandler； 然后，以接口Hello为纽带，为被调用目标构建代理对象， 进而应用程序就可以使用代理对象间接运行调用目标的逻辑，代理为应用插入额外逻辑（这里是println）提供了便利的入口。 从API设计和实现的角度，这种实现仍然有局限性，因为它是以接口为中心的，相当于添加了一种对于被调用者没有太大意义的限制。我们实例化的是Proxy对象，而不是真正的被调用类型，这在实践中还是可能带来各种不便和能力退化。 如果被调用者没有实现接口，而我们还是希望利用动态代理机制，那么可以考虑其他方式。我们知道Spring AOP支持两种模式的动态代理，JDK Proxy或者cglib，如果我们选择cglib方式，你会发现对接口的依赖被克服了。 cglib动态代理采取的是创建目标类的子类的方式，因为是子类化，我们可以达到近似使用被调用者本身的效果。在Spring编程中，框架通常会处理这种情况，当然我们也可以显式指定。关于类似方案的实现细节，我就不再详细讨论了。 那我们在开发中怎样选择呢？我来简单对比下两种方式各自优势。JDK Proxy的优势： 最小化依赖关系，减少依赖意味着简化开发和维护，JDK本身的支持，可能比cglib更加可靠。 平滑进行JDK版本升级，而字节码类库通常需要进行更新以保证在新版Java上能够使用。 代码实现简单。 基于类似cglib框架的优势： 有的时候调用目标可能不便实现额外接口，从某种角度看，限定调用者实现接口是有些侵入性的实践，类似cglib动态代理就没有这种限制。 只操作我们关心的类，而不必为其他相关类增加工作量。 高性能。 另外，从性能角度，我想补充几句。记得有人曾经得出结论说JDK Proxy比cglib或者Javassist慢几十倍。坦白说，不去争论具体的benchmark细节，在主流JDK版本中，JDK Proxy在典型场景可以提供对等的性能水平，数量级的差距基本上不是广泛存在的。而且，反射机制性能在现代JDK中，自身已经得到了极大的改进和优化，同时，JDK很多功能也不完全是反射，同样使用了ASM进行字节码操作。 我们在选型中，性能未必是唯一考量，可靠性、可维护性、编程工作量等往往是更主要的考虑因素，毕竟标准类库和反射编程的门槛要低得多，代码量也是更加可控的，如果我们比较下不同开源项目在动态代理开发上的投入，也能看到这一点。 动态代理应用非常广泛，虽然最初多是因为RPC等使用进入我们视线，但是动态代理的使用场景远远不仅如此，它完美符合Spring AOP等切面编程。我在后面的专栏还会进一步详细分析AOP的目的和能力。简单来说它可以看作是对OOP的一个补充，因为OOP对于跨越不同对象或类的分散、纠缠逻辑表现力不够，比如在不同模块的特定阶段做一些事情，类似日志、用户鉴权、全局性异常处理、性能监控，甚至事务处理等，你可以参考下面这张图。AOP通过（动态）代理机制可以让开发者从这些繁琐事项中抽身出来，大幅度提高了代码的抽象程度和复用度。从逻辑上来说，我们在软件设计和实现中的类似代理，如Facade、Observer等很多设计目的，都可以通过动态代理优雅地实现。 今天 我简要回顾了反射机制， 谈了反射在Java语言演进中正在发生的变化， 并且进一步探讨了动态代理机制和相关的切面编程，分析了其解决的问题，并探讨了生产实践中的选择考量。 一课一练关于今天我们讨论的题目你做到心中有数了吗？留一道思考题给你，你在工作中哪些场景使用到了动态代理？相应选择了什么实现技术？选择的依据是什么？ 请你在留言区写写你对这个问题的思考，我会选出经过认真思考的留言，送给你一份学习鼓励金，欢迎你与我一起讨论。你的朋友是不是也在准备面试呢？你可以“请朋友读”，把今天的题目分享给好友，或许你能帮到他。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础专题</category>
      </categories>
      <tags>
        <tag>阅读笔记</tag>
        <tag>Java核心36讲</tag>
        <tag>极客的专栏</tag>
        <tag>动态代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02Exception和Error有什么区别[笔记]]]></title>
    <url>%2F2020%2F03%2F07%2FJava%E6%A0%B8%E5%BF%8336%E8%AE%B202%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[NoClassDefFoundError和ClassNotFoundException有什么区别 NoClassDefFoundError错误发生的原因NoClassDefFoundError错误的发生，是因为Java虚拟机在编译时能找到合适的类，而在运行时不能找到合适的类导致的错误。例如在运行时我们想调用某个类的方法或者访问这个类的静态成员的时候，发现这个类不可用，此时Java虚拟机就会抛出NoClassDefFoundError错误。与ClassNotFoundException的不同在于，这个错误发生只在运行时需要加载对应的类不成功，而不是编译时发生。很多Java开发者很容易在这里把这两个错误搞混。简单总结就是，NoClassDefFoundError发生在编译时对应的类可用，而运行时在Java的classpath路径中，对应的类不可用导致的错误。 NoClassDefFoundError和ClassNotFoundException区别我们经常被java.lang.ClassNotFoundException和java.lang.NoClassDefFoundError这两个错误迷惑不清，尽管他们都与Java classpath有关，但是他们完全不同。NoClassDefFoundError发生在JVM在动态运行时，根据你提供的类名，在classpath中找到对应的类进行加载，但当它找不到这个类时，就发生了java.lang.NoClassDefFoundError的错误，而ClassNotFoundException是在编译的时候在classpath中找不到对应的类而发生的错误。ClassNotFoundException比NoClassDefFoundError容易解决，是因为在编译时我们就知道错误发生，并且完全是由于环境的问题导致。而如果你在J2EE的环境下工作，并且得到NoClassDefFoundError的异常，而且对应的错误的类是确实存在的，这说明这个类对于类加载器来说，可能是不可见的。 怎么解决NoClassDefFoundError错误根据前文，很明显NoClassDefFoundError的错误是因为在运行时类加载器在classpath下找不到需要加载的类，所以我们需要把对应的类加载到classpath中，或者检查为什么类在classpath中是不可用的，这个发生可能的原因如下： 对应的Class在java的classpath中不可用 你可能用jar命令运行你的程序，但类并没有在jar文件的manifest文件中的classpath属性中定义 可能程序的启动脚本覆盖了原来的classpath环境变量 因为NoClassDefFoundError是java.lang.LinkageError的一个子类，所以可能由于程序依赖的原生的类库不可用而导致 检查日志文件中是否有java.lang.ExceptionInInitializerError这样的错误，NoClassDefFoundError有可能是由于静态初始化失败导致的（这是我遇到的问题的解决办法） 如果你工作在J2EE的环境，有多个不同的类加载器，也可能导致NoClassDefFoundError。 NoClassDefFoundError也可能由于类的静态初始化模块错误导致，当你的类执行一些静态初始化模块操作，如果初始化模块抛出异常，哪些依赖这个类的其他类会抛出NoClassDefFoundError的错误。如果你查看程序日志，会发现一些java.lang.ExceptionInInitializerError的错误日志，ExceptionInInitializerError的错误会导致java.lang.NoClassDefFoundError: Could not initialize class 案例项目启动时报错NoClassDefFoundError:DataSourceFactorydebug断点启动可以看到DataSourceFactory加载static时已报错]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础专题</category>
      </categories>
      <tags>
        <tag>阅读笔记</tag>
        <tag>Java核心36讲笔记</tag>
        <tag>异常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05String、StringBuffer、StringBuilder有什么区别？[转]]]></title>
    <url>%2F2020%2F03%2F06%2FJava%E6%A0%B8%E5%BF%8336%E8%AE%B205%2F</url>
    <content type="text"><![CDATA[今天我会聊聊日常使用的字符串，别看它似乎很简单，但其实字符串几乎在所有编程语言里都是个特殊的存在，因为不管是数量还是体积，字符串都是大多数应用中的重要组成。今天我要问你的问题是，理解Java的字符串，String、StringBuffer、StringBuilder有什么区别？ 典型回答String是Java语言非常基础和重要的类，提供了构造和管理字符串的各种基本逻辑。它是典型的Immutable类，被声明成为final class，所有属性也都是final的。也由于它的不可变性，类似拼接、裁剪字符串等动作，都会产生新的String对象。由于字符串操作的普遍性，所以相关操作的效率往往对应用性能有明显影响。 StringBuffer是为解决上面提到拼接产生太多中间对象的问题而提供的一个类，我们可以用append或者add方法，把字符串添加到已有序列的末尾或者指定位置。StringBuffer本质是一个线程安全的可修改字符序列，它保证了线程安全，也随之带来了额外的性能开销，所以除非有线程安全的需要，不然还是推荐使用它的后继者，也就是StringBuilder。 StringBuilder是Java 1.5中新增的，在能力上和StringBuffer没有本质区别，但是它去掉了线程安全的部分，有效减小了开销，是绝大部分情况下进行字符串拼接的首选。 考点分析几乎所有的应用开发都离不开操作字符串，理解字符串的设计和实现以及相关工具如拼接类的使用，对写出高质量代码是非常有帮助的。关于这个问题，我前面的回答是一个通常的概要性回答，至少你要知道String是Immutable的，字符串操作不当可能会产生大量临时字符串，以及线程安全方面的区别。 如果继续深入，面试官可以从各种不同的角度考察，比如可以： 通过String和相关类，考察基本的线程安全设计与实现，各种基础编程实践。 考察JVM对象缓存机制的理解以及如何良好地使用。 考察JVM优化Java代码的一些技巧。 String相关类的演进，比如Java 9中实现的巨大变化。 … 针对上面这几方面，我会在知识扩展部分与你详细聊聊。 知识扩展1.字符串设计和实现考量我在前面介绍过，String是Immutable类的典型实现，原生的保证了基础线程安全，因为你无法对它内部数据进行任何修改，这种便利甚至体现在拷贝构造函数中，由于不可变，Immutable对象在拷贝时不需要额外复制数据。 我们再来看看StringBuffer实现的一些细节，它的线程安全是通过把各种修改数据的方法都加上synchronized关键字实现的，非常直白。其实，这种简单粗暴的实现方式，非常适合我们常见的线程安全类实现，不必纠结于synchronized性能之类的，有人说“过早优化是万恶之源”，考虑可靠性、正确性和代码可读性才是大多数应用开发最重要的因素。 为了实现修改字符序列的目的，StringBuffer和StringBuilder底层都是利用可修改的（char，JDK 9以后是byte）数组，二者都继承了AbstractStringBuilder，里面包含了基本操作，区别仅在于最终的方法是否加了synchronized。 另外，这个内部数组应该创建成多大的呢？如果太小，拼接的时候可能要重新创建足够大的数组；如果太大，又会浪费空间。目前的实现是，构建时初始字符串长度加16（这意味着，如果没有构建对象时输入最初的字符串，那么初始值就是16）。我们如果确定拼接会发生非常多次，而且大概是可预计的，那么就可以指定合适的大小，避免很多次扩容的开销。扩容会产生多重开销，因为要抛弃原有数组，创建新的（可以简单认为是倍数）数组，还要进行arraycopy。 前面我讲的这些内容，在具体的代码书写中，应该如何选择呢？ 在没有线程安全问题的情况下，全部拼接操作是应该都用StringBuilder实现吗？毕竟这样书写的代码，还是要多敲很多字的，可读性也不理想，下面的对比非常明显。 String strByBuilder = new StringBuilder().append(&quot;aa&quot;).append(&quot;bb&quot;).append(&quot;cc&quot;).append(&quot;dd&quot;).toString(); String strByConcat = &quot;aa&quot; + &quot;bb&quot; + &quot;cc&quot; + &quot;dd&quot;; 其实，在通常情况下，没有必要过于担心，要相信Java还是非常智能的。我们来做个实验，把下面一段代码，利用不同版本的JDK编译，然后再反编译，例如： public class StringConcat { public satic void main(String[] args) { String myStr = &quot;aa&quot; + &quot;bb&quot; + &quot;cc&quot; + &quot;dd&quot;; System.out.println(&quot;My String:&quot; + myStr); } } 先编译再反编译，比如使用JDK 9： ${JAVA9_HOME}/bin/javac StringConcat.java ${JAVA9_HOME}/bin/javap -v StringConcat.class JDK 8的输出片段是： 6: new #4 // class java/lang/StringBuilder 9: dup 10: invokespecial #5 // Method java/lang/StringBuilder.&quot;&lt;init&gt;&quot;:()V 13: ldc #6 // String My String: 15: invokevirtual #7 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 18: aload_1 19: invokevirtual #7 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 22: invokevirtual #8 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 而在JDK 9中，反编译的结果就非常简单了，片段是： 7: invokedynamic #4, 0 // InvokeDynamic #0:makeConcatWithConsants:(Ljava/lang/String;)Ljava/lang/String; 你可以看到，在JDK 8中，字符串拼接操作会自动被javac转换为StringBuilder操作，而在JDK 9里面则是因为Java 9为了更加统一字符串操作优化，提供了StringConcatFactory，作为一个统一的入口。javac自动生成的代码，虽然未必是最优化的，但普通场景也足够了，你可以酌情选择。 2.字符串缓存我们粗略统计过，把常见应用进行堆转储（Dump Heap），然后分析对象组成，会发现平均25%的对象是字符串，并且其中约半数是重复的。如果能避免创建重复字符串，可以有效降低内存消耗和对象创建开销。 String在Java 6以后提供了intern()方法，目的是提示JVM把相应字符串缓存起来，以备重复使用。在我们创建字符串对象并调用intern()方法的时候，如果已经有缓存的字符串，就会返回缓存里的实例，否则将其缓存起来。 一般来说，JVM会将所有的类似“abc”这样的文本字符串，或者字符串常量之类缓存起来。 看起来很不错是吧？但实际情况估计会让你大跌眼镜。一般使用Java 6这种历史版本，并不推荐大量使用intern，为什么呢？魔鬼存在于细节中，被缓存的字符串是存在所谓PermGen里的，也就是臭名昭著的“永久代”，这个空间是很有限的，也基本不会被FullGC之外的垃圾收集照顾到。所以，如果使用不当，OOM就会光顾。 在后续版本中，这个缓存被放置在堆中，这样就极大避免了永久代占满的问题，甚至永久代在JDK 8中被MetaSpace（元数据区）替代了。而且，默认缓存大小也在不断地扩大中，从最初的1009，到7u40以后被修改为60013。你可以使用下面的参数直接打印具体数字，可以拿自己的JDK立刻试验一下。 -XX:+PrintStringTableStatisics 你也可以使用下面的JVM参数手动调整大小，但是绝大部分情况下并不需要调整，除非你确定它的大小已经影响了操作效率。 -XX:StringTableSize=N Intern是一种显式地排重机制，但是它也有一定的副作用，因为需要开发者写代码时明确调用，一是不方便，每一个都显式调用是非常麻烦的；另外就是我们很难保证效率，应用开发阶段很难清楚地预计字符串的重复情况，有人认为这是一种污染代码的实践。 幸好在Oracle JDK 8u20之后，推出了一个新的特性，也就是G1 GC下的字符串排重。它是通过将相同数据的字符串指向同一份数据来做到的，是JVM底层的改变，并不需要Java类库做什么修改。 注意这个功能目前是默认关闭的，你需要使用下面参数开启，并且记得指定使用G1 GC： -XX:+UseStringDeduplication 前面说到的几个方面，只是Java底层对字符串各种优化的一角，在运行时，字符串的一些基础操作会直接利用JVM内部的Intrinsic机制，往往运行的就是特殊优化的本地代码，而根本就不是Java代码生成的字节码。Intrinsic可以简单理解为，是一种利用native方式hard-coded的逻辑，算是一种特别的内联，很多优化还是需要直接使用特定的CPU指令，具体可以看相关源码，搜索“string”以查找相关Intrinsic定义。当然，你也可以在启动实验应用时，使用下面参数，了解intrinsic发生的状态。 -XX:+PrintCompilation -XX:+UnlockDiagnosicVMOptions -XX:+PrintInlining //样例输出片段 180 3 3 java.lang.String::charAt (25 bytes) @ 1 java.lang.String::isLatin1 (19 bytes) ... @ 7 java.lang.StringUTF16::getChar (60 bytes) intrinsic 可以看出，仅仅是字符串一个实现，就需要Java平台工程师和科学家付出如此大且默默无闻的努力，我们得到的很多便利都是来源于此。我会在专栏后面的JVM和性能等主题，详细介绍JVM内部优化的一些方法，如果你有兴趣可以再深入学习。即使你不做JVM开发或者暂时还没有使用到特别的性能优化，这些知识也能帮助你增加技术深度。3.String自身的演化如果你仔细观察过Java的字符串，在历史版本中，它是使用char数组来存数据的，这样非常直接。但是Java中的char是两个bytes大小，拉丁语系语言的字符，根本就不需要太宽的char，这样无区别的实现就造成了一定的浪费。密度是编程语言平台永恒的话题，因为归根结底绝大部分任务是要来操作数据的。其实在Java 6的时候，Oracle JDK就提供了压缩字符串的特性，但是这个特性的实现并不是开源的，而且在实践中也暴露出了一些问题，所以在最新的JDK版本中已经将它移除了。 在Java 9中，我们引入了Compact Strings的设计，对字符串进行了大刀阔斧的改进。将数据存储方式从char数组，改变为一个byte数组加上一个标识编码的所谓coder，并且将相关字符串操作类都进行了修改。另外，所有相关的Intrinsic之类也都进行了重写，以保证没有任何性能损失。虽然底层实现发生了这么大的改变，但是Java字符串的行为并没有任何大的变化，所以这个特性对于绝大部分应用来说是透明的，绝大部分情况不需要修改已有代码。当然，在极端情况下，字符串也出现了一些能力退化，比如最大字符串的大小。你可以思考下，原来char数组的实现，字符串的最大长度就是数组本身的长度限制，但是替换成byte数组，同样数组长度下，存储能力是退化了一倍的！还好这是存在于理论中的极限，还没有发现现实应用受此影响。在通用的性能测试和产品实验中，我们能非常明显地看到紧凑字符串带来的优势，即更小的内存占用、更快的操作速度。 今天 我从String、StringBuffer和StringBuilder的主要设计和实现特点开始，分析了字符串缓存的intern机制、非代码侵入性的虚拟机层面排重、Java 9中紧凑字符的改进， 并且初步接触了JVM的底层优化机制intrinsic。 从实践的角度，不管是Compact Strings还是底层intrinsic优化，都说明了使用Java基础类库的优势，它们往往能够得到最大程度、最高质量的优化，而且只要升级JDK版本，就能零成本地享受这些益处。 一课一练关于今天我们讨论的题目你做到心中有数了吗？限于篇幅有限，还有很多字符相关的问题没有来得及讨论，比如编码相关的问题。可以思考一下，很多字符串操作，比如getBytes()/String(byte[] bytes)等都是隐含着使用平台默认编码，这是一种好的实践吗？是否有利于避免乱码？ 请你在留言区写写你对这个问题的思考，或者分享一下你在操作字符串时掉过的坑，我会选出经过认真思考的留言，送给你一份学习鼓励金，欢迎你与我一起讨论。你的朋友是不是也在准备面试呢？你可以“请朋友读”，把今天的题目分享给好友，或许你能帮到他。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础专题</category>
      </categories>
      <tags>
        <tag>阅读笔记</tag>
        <tag>Java核心36讲</tag>
        <tag>极客的专栏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04强引用、软引用、弱引用、幻象引用有什么区别？[转]]]></title>
    <url>%2F2020%2F03%2F05%2FJava%E6%A0%B8%E5%BF%8336%E8%AE%B204%2F</url>
    <content type="text"><![CDATA[在Java语言中，除了原始数据类型的变量，其他所有都是所谓的引用类型，指向各种不同的对象，理解引用对于掌握Java对象生命周期和JVM内部相关机制非常有帮助。今天我要问你的问题是，强引用、软引用、弱引用、幻象引用有什么区别？具体使用场景是什么？ 典型回答不同的引用类型，主要体现的是对象不同的可达性（reachable）状态和对垃圾收集的影响。 所谓强引用（”Strong” Reference），就是我们最常见的普通对象引用，只要还有强引用指向一个对象，就能表明对象还“活着”，垃圾收集器不会碰这种对象。对于一个普通的对象，如果没有其他的引用关系，只要超过了引用的作用域或者显式地将相应（强）引用赋值为null，就是可以被垃圾收集的了，当然具体回收时机还是要看垃圾收集策略。 软引用（SoftReference），是一种相对强引用弱化一些的引用，可以让对象豁免一些垃圾收集，只有当JVM认为内存不足时，才会去试图回收软引用指向的对象。JVM会确保在抛出OutOfMemoryError之前，清理软引用指向的对象。软引用通常用来实现内存敏感的缓存，如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。 弱引用（WeakReference）并不能使对象豁免垃圾收集，仅仅是提供一种访问在弱引用状态下对象的途径。这就可以用来构建一种没有特定约束的关系，比如，维护一种非强制性的映射关系，如果试图获取时对象还在，就使用它，否则重现实例化。它同样是很多缓存实现的选择。 对于幻象引用，有时候也翻译成虚引用，你不能通过它访问对象。幻象引用仅仅是提供了一种确保对象被finalize以后，做某些事情的机制，比如，通常用来做所谓的PostMortem清理机制，我在专栏上一讲中介绍的Java平台自身Cleaner机制等，也有人利用幻象引用监控对象的创建和销毁。 考点分析这道面试题，属于既偏门又非常高频的一道题目。说它偏门，是因为在大多数应用开发中，很少直接操作各种不同引用，虽然我们使用的类库、框架可能利用了其机制。它被频繁问到，是因为这是一个综合性的题目，既考察了我们对基础概念的理解，也考察了对底层对象生命周期、垃圾收集机制等的掌握。 充分理解这些引用，对于我们设计可靠的缓存等框架，或者诊断应用OOM等问题，会很有帮助。比如，诊断MySQL connector-j驱动在特定模式下（useCompression=true）的内存泄漏问题，就需要我们理解怎么排查幻象引用的堆积问题。 知识扩展1.对象可达性状态流转分析 首先，请你看下面流程图，我这里简单总结了对象生命周期和不同可达性状态，以及不同状态可能的改变关系，可能未必100%严谨，来阐述下可达性的变化。 我来解释一下上图的具体状态，这是Java定义的不同可达性级别（reachability level），具体如下： 强可达（Strongly Reachable），就是当一个对象可以有一个或多个线程可以不通过各种引用访问到的情况。比如，我们新创建一个对象，那么创建它的线程对它就是强可达。 软可达（Softly Reachable），就是当我们只能通过软引用才能访问到对象的状态。 弱可达（Weakly Reachable），类似前面提到的，就是无法通过强引用或者软引用访问，只能通过弱引用访问时的状态。这是十分临近finalize状态的时机，当弱引用被清除的时候，就符合finalize的条件了。 幻象可达（Phantom Reachable），上面流程图已经很直观了，就是没有强、软、弱引用关联，并且finalize过了，只有幻象引用指向这个对象的时候。 当然，还有一个最后的状态，就是不可达（unreachable），意味着对象可以被清除了。 判断对象可达性，是JVM垃圾收集器决定如何处理对象的一部分考虑。所有引用类型，都是抽象类java.lang.ref.Reference的子类，你可能注意到它提供了get()方法： T get() Return this reference object&apos;s referent 除了幻象引用（因为get永远返回null），如果对象还没有被销毁，都可以通过get方法获取原有对象。这意味着，利用软引用和弱引用，我们可以将访问到的对象，重新指向强引用，也就是人为的改变了对象的可达性状态！这也是为什么我在上面图里有些地方画了双向箭头。 所以，对于软引用、弱引用之类，垃圾收集器可能会存在二次确认的问题，以保证处于弱引用状态的对象，没有改变为强引用。 但是，你觉得这里有没有可能出现什么问题呢？ 不错，如果我们错误的保持了强引用（比如，赋值给了static变量），那么对象可能就没有机会变回类似弱引用的可达性状态了，就会产生内存泄漏。所以，检查弱引用指向对象是否被垃圾收集，也是诊断是否有特定内存泄漏的一个思路，如果我们的框架使用到弱引用又怀疑有内存泄漏，就可以从这个角度检查。 2.引用队列（ReferenceQueue）使用 谈到各种引用的编程，就必然要提到引用队列。 我们在创建各种引用并关联到响应对象时，可以选择是否需要关联引用队列，JVM会在特定时机将引用enqueue到队列里，我们可以从队列里获取引用（remove方法在这里实际是有获取的意思）进行相关后续逻辑。尤其是幻象引用，get方法只返回null，如果再不指定引用队列，基本就没有意义了。看看下面的示例代码。利用引用队列，我们可以在对象处于相应状态时（对于幻象引用，就是前面说的被finalize了，处于幻象可达状态），执行后期处理逻辑。 Object counter = new Object(); ReferenceQueue refQueue = new ReferenceQueue&lt;&gt;(); PhantomReference&lt;Object&gt; p = new PhantomReference&lt;&gt;(counter, refQueue); counter = null; Sysem.gc(); try { // Remove是一个阻塞方法，可以指定timeout，或者选择一直阻塞 Reference&lt;Object&gt; ref = refQueue.remove(1000L); if (ref != null) { // do something } } catch (InterruptedException e) { // Handle it } 3.显式地影响软引用垃圾收集 前面泛泛提到了引用对垃圾收集的影响，尤其是软引用，到底JVM内部是怎么处理它的，其实并不是非常明确。那么我们能不能使用什么方法来影响软引用的垃圾收集呢？答案是有的。软引用通常会在最后一次引用后，还能保持一段时间，默认值是根据堆剩余空间计算的（以M bytes为单位）。从Java 1.3.1开始，提供了-XX:SoftRefLRUPolicyMSPerMB参数，我们可以以毫秒（milliseconds）为单位设置。比如，下面这个示例就是设置为3秒（3000毫秒）。 -XX:SoftRefLRUPolicyMSPerMB=3000这个剩余空间，其实会受不同JVM模式影响，对于Client模式，比如通常的Windows 32 bit JDK，剩余空间是计算当前堆里空闲的大小，所以更加倾向于回收；而对于server模 式JVM，则是根据-Xmx指定的最大值来计算。本质上，这个行为还是个黑盒，取决于JVM实现，即使是上面提到的参数，在新版的JDK上也未必有效，另外Client模式的JDK已经逐步退出历史舞台。所以在我们应用时，可以参考类似设置，但不要过于依赖它。 4.诊断JVM引用情况 如果你怀疑应用存在引用（或finalize）导致的回收问题，可以有很多工具或者选项可供选择，比如HotSpot JVM自身便提供了明确的选项（PrintReferenceGC）去获取相关信息，我指定了下面选项去使用JDK 8运行一个样例应用： -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintReferenceGC 这是JDK 8使用ParrallelGC收集的垃圾收集日志，各种引用数量非常清晰。 0.403: [GC (Allocation Failure) 0.871: [SoftReference, 0 refs, 0.0000393 secs]0.871: [WeakReference, 8 refs, 0.0000138 secs]0.871: [FinalReference, 4 refs, 0.0000094 secs]0.871: [PhantomReference, 0 refs, 0 refs, 0.0000085 secs]0.871: [JNI Weak Reference, 0.0000071 secs][PSYoungGen: 76272K-&gt;10720K(141824K)] 128286K-&gt;128422K(316928K), 0.4683919 secs] [Times: user=1.17 sys=0.03, real=0.47 secs] 注意：JDK 9对JVM和垃圾收集日志进行了广泛的重构，类似PrintGCTimeStamps和PrintReferenceGC已经不再存在，我在专栏后面的垃圾收集主题里会更加系统的阐述。 5.Reachability Fence 除了我前面介绍的几种基本引用类型，我们也可以通过底层API来达到强引用的效果，这就是所谓的设置reachability fence。为什么需要这种机制呢？考虑一下这样的场景，按照Java语言规范，如果一个对象没有指向强引用，就符合垃圾收集的标准，有些时候，对象本身并没有强引用，但是也许它的部分属性还在被使用，这样就导致诡异的问题，所以我们需要一个方法，在没有强引用情况下，通知JVM对象是在被使用的。说起来有点绕，我们来看看Java 9中提供的案例。 class Resource { private satic ExternalResource[] externalResourceArray = ... int myIndex; Resource(...) { myIndex = ... externalResourceArray[myIndex] = ...; ... } protected void finalize() { externalResourceArray[myIndex] = null; ... } public void action() { try { // 需要被保护的代码 int i = myIndex; Resource.update(externalResourceArray[i]); } finally { // 调用reachbilityFence，明确保障对象srongly reachable Reference.reachabilityFence(this); } } private satic void update(ExternalResource ext) { ext.satus = ...; } } 方法action的执行，依赖于对象的部分属性，所以被特定保护了起来。否则，如果我们在代码中像下面这样调用，那么就可能会出现困扰，因为没有强引用指向我们创建出来的Resource对象，JVM对它进行finalize操作是完全合法的。 new Resource().action() 类似的书写结构，在异步编程中似乎是很普遍的，因为异步编程中往往不会用传统的“执行-&gt;返回-&gt;使用”的结构。在Java 9之前，实现类似类似功能相对比较繁琐，有的时候需要采取一些比较隐晦的小技巧。幸好，java.lang.ref.Reference给我们提供了新方法，它是JEP 193: VariableHandles的一部分，将Java平台底层的一些能力暴露出来： satic void reachabilityFence(Object ref) 在JDK源码中，reachabilityFence大多使用在Executors或者类似新的HTTP/2客户端代码中，大部分都是异步调用的情况。编程中，可以按照上面这个例子，将需要reachability保障的代码段利用try-finally包围起来，在finally里明确声明对象强可达。 今天， 我总结了Java语言提供的几种引用类型、相应可达状态以及对于JVM工作的意义， 并分析了引用队列使用的一些实际情况， 最后介绍了在新的编程模式下，如何利用API去保障对象不被以为意外回收，希望对你有所帮助。 一课一练关于今天我们讨论的题目你做到心中有数了吗？给你留一道练习题，你能从自己的产品或者第三方类库中找到使用各种引用的案例吗？它们都试图解决什么问题？ 请你在留言区写写你的答案，我会选出经过认真思考的留言，送给你一份学习鼓励金，欢迎你与我一起讨论。你的朋友是不是也在准备面试呢？你可以“请朋友读”，把今天的题目分享出去，或许你能帮到他。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础专题</category>
      </categories>
      <tags>
        <tag>阅读笔记</tag>
        <tag>Java核心36讲</tag>
        <tag>极客的专栏</tag>
        <tag>对象引用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01谈谈你对Java平台的理解?[笔记]]]></title>
    <url>%2F2020%2F03%2F04%2FJava%E6%A0%B8%E5%BF%8336%E8%AE%B201%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[谈谈你对Java平台的理解？“Java是解释执行”，这句话正确吗？ Java的两个特性 跨平台能力 垃圾收集（GC, Garbage Collection） Java的解释执行与编译执行 解释执行 首先，Java的源代码通过Javac编译成为字节码（bytecode）， 然后，在运行时，通过 Java虚拟机（JVM）内嵌的解释器将字节码转换成为最终的机器码。 编译执行 常见的JVM，比如我们大多数情况使用的Oracle JDK提供的Hotspot JVM，都提供了JIT（Just-In-Time）编译器，也就是通常所说的动态编译器，JIT能够在运行时将热点代码编译成机器码，这种情况下部分热点代码就属于编译执行，而不是解释执行了。 Java虚拟机启动时，可以指定不同的参数对运行模式进行选择。 指定“-Xint”,只进行解释执行，不对代码进行编译 “-Xcomp”参数,告诉JVM关闭解释器，不要进行解释执行，或者叫作最大优化级别 AOT（Ahead-of-Time Compilation），直接将字节码编译成机器代码 Java的三种编译方式 Java语言平台包括 Java语言特性，包括泛型、Lambda等语言特性； 基础类库，包括集合、IO/NIO、网络、并发、安全等基础类库。对于我们日常工作应用较多的类库，面试前可以系统化总结一下，有助于临场发挥。 或者谈谈JVM的一些基础概念和机制，比如 Java的类加载机制，常用版本JDK（如JDK 8）内嵌的Class-Loader，例如Bootstrap、 Application和Extension Class-loader； 类加载大致过程：加载、验证、链接、初始化（这里参考了周志明的《深入理解Java虚拟机》，非常棒的JVM上手书籍）； 自定义Class-Loader等。 还有垃圾收集的基本原理，最常见的垃圾收集器，如SerialGC、Parallel GC、 CMS、 G1等，对于适用于什么样的工作负载最好也心里有数。这些都是可以扩展开的领域，我会在后面的专栏对此进行更系统的介绍。 当然还有JDK包含哪些工具或者Java领域内其他工具等，如编译器、运行时环境、安全工具、诊断和监控工具等。这些基本工具是日常工作效率的保证，对于我们工作在其他语言平台上，同样有所帮助，很多都是触类旁通的。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础专题</category>
      </categories>
      <tags>
        <tag>阅读笔记</tag>
        <tag>Java核心36讲笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03谈谈final、finally、 finalize有什么不同？[转]]]></title>
    <url>%2F2020%2F03%2F03%2FJava%E6%A0%B8%E5%BF%8336%E8%AE%B203%2F</url>
    <content type="text"><![CDATA[Java语言有很多看起来很相似，但是用途却完全不同的语言要素，这些内容往往容易成为面试官考察你知识掌握程度的切入点。今天，我要问你的是一个经典的Java基础题目，谈谈final、finally、 finalize有什么不同？ 典型回答 final可以用来修饰类、方法、变量，分别有不同的意义，final修饰的class代表不可以继承扩展，final的变量是不可以修改的，而final的方法也是不可以重写的（override）。 finally则是Java保证重点代码一定要被执行的一种机制。我们可以使用try-finally或者try-catch-finally来进行类似关闭JDBC连接、保证unlock锁等动作。 finalize是基础类java.lang.Object的一个方法，它的设计目的是保证对象在被垃圾收集前完成特定资源的回收。finalize机制现在已经不推荐使用，并且在JDK 9开始被标记为deprecated。 考点分析这是一个非常经典的Java基础问题，我上面的回答主要是从语法和使用实践角度出发的，其实还有很多方面可以深入探讨，面试官还可以考察你对性能、并发、对象生命周期或垃圾收集基本过程等方面的理解。 推荐使用final关键字来明确表示我们代码的语义、逻辑意图，这已经被证明在很多场景下是非常好的实践，比如： 我们可以将方法或者类声明为final，这样就可以明确告知别人，这些行为是不许修改的。 如果你关注过Java核心类库的定义或源码， 有没有发现java.lang包下面的很多类，相当一部分都被声明成为final class,在第三方类库的一些基础类中同样如此，这可以有效避免API使用者更改基础功能，某种程度上，这是保证平台安全的必要手段。 使用final修饰参数或者变量，也可以清楚地避免意外赋值导致的编程错误，甚至，有人明确推荐将所有方法参数、本地变量、成员变量声明成final。 final变量产生了某种程度的不可变（immutable）的效果，所以，可以用于保护只读数据，尤其是在并发编程中，因为明确地不能再赋值final变量，有利于减少额外的同步开销，也可以省去一些防御性拷贝的必要。 final也许会有性能的好处，很多文章或者书籍中都介绍了可在特定场景提高性能，比如，利用final可能有助于JVM将方法进行内联，可以改善编译器进行条件编译的能力等等。坦白说，很多类似的结论都是基于假设得出的，比如现代高性能JVM（如HotSpot）判断内联未必依赖final的提示，要相信JVM还是非常智能的。类似的，final字段对性能的影响，大部分情况下，并没有考虑的必要。 从开发实践的角度，我不想过度强调这一点，这是和JVM的实现很相关的，未经验证比较难以把握。我的建议是，在日常开发中，除非有特别考虑，不然最好不要指望这种小技巧带来的所谓性能好处，程序最好是体现它的语义目的。如果你确实对这方面有兴趣，可以查阅相关资料，我就不再赘述了，不过千万别忘了验证一下。 对于finally，明确知道怎么使用就足够了。需要关闭的连接等资源，更推荐使用Java 7中添加的try-with-resources语句，因为通常Java平台能够更好地处理异常情况，编码量也要少很多，何乐而不为呢。另外，我注意到有一些常被考到的finally问题（也比较偏门），至少需要了解一下。比如，下面代码会输出什么？ try { // do something Sysem.exit(1); } finally{ Sysem.out.println(“Print from finally”); } 上面finally里面的代码可不会被执行的哦，这是一个特例。 对于finalize，我们要明确它是不推荐使用的，业界实践一再证明它不是个好的办法，在Java 9中，甚至明确将Object.finalize()标记为deprecated！如果没有特别的原因，不要实现finalize方法，也不要指望利用它来进行资源回收。为什么呢？简单说，你无法保证finalize什么时候执行，执行的是否符合预期。使用不当会影响性能，导致程序死锁、挂起等。 通常来说，利用上面的提到的try-with-resources或者try-finally机制，是非常好的回收资源的办法。如果确实需要额外处理，可以考虑Java提供的Cleaner机制或者其他替代方法。接下来，我来介绍更多设计考虑和实践细节。 知识扩展1.注意，final不是immutable！我在前面介绍了final在实践中的益处，需要注意的是，final并不等同于immutable，比如下面这段代码： final List&lt;String&gt; strList = new ArrayList&lt;&gt;(); strList.add(&quot;Hello&quot;); strList.add(&quot;world&quot;); List&lt;String&gt; unmodifableStrList = List.of(&quot;hello&quot;, &quot;world&quot;); unmodifableStrList.add(&quot;again&quot;); final只能约束strList这个引用不可以被赋值，但是strList对象行为不被final影响，添加元素等操作是完全正常的。如果我们真的希望对象本身是不可变的，那么需要相应的类支持不可变的行为。在上面这个例子中，List.of方法创建的本身就是不可变List，最后那句add是会在运行时抛出异常的。 Immutable在很多场景是非常棒的选择，某种意义上说，Java语言目前并没有原生的不可变支持，如果要实现immutable的类，我们需要做到： 将class自身声明为final，这样别人就不能扩展来绕过限制了。 将所有成员变量定义为private和final，并且不要实现setter方法。 通常构造对象时，成员变量使用深度拷贝来初始化，而不是直接赋值，这是一种防御措施，因为你无法确定输入对象不被其他人修改。 如果确实需要实现getter方法，或者其他可能会返回内部状态的方法，使用copy-on-write原则，创建私有的copy。 这些原则是不是在并发编程实践中经常被提到？的确如此。 关于setter/getter方法，很多人喜欢直接用IDE一次全部生成，建议最好是你确定有需要时再实现。 2.finalize真的那么不堪？前面简单介绍了finalize是一种已经被业界证明了的非常不好的实践，那么为什么会导致那些问题呢？ finalize的执行是和垃圾收集关联在一起的，一旦实现了非空的finalize方法，就会导致相应对象回收呈现数量级上的变慢，有人专门做过benchmark，大概是40~50倍的下降。因为，finalize被设计成在对象被垃圾收集前调用，这就意味着实现了finalize方法的对象是个“特殊公民”，JVM要对它进行额外处理。finalize本质上成为了快速回收的阻碍者，可能导致你的对象经过多个垃圾收集周期才能被回收。 有人也许会问，我用System.runFinalization()告诉JVM积极一点，是不是就可以了？也许有点用，但是问题在于，这还是不可预测、不能保证的，所以本质上还是不能指望。实践中，因为finalize拖慢垃圾收集，导致大量对象堆积，也是一种典型的导致OOM的原因。 从另一个角度，我们要确保回收资源就是因为资源都是有限的，垃圾收集时间的不可预测，可能会极大加剧资源占用。这意味着对于消耗非常高频的资源，千万不要指望finalize去承担资源释放的主要职责，最多让finalize作为最后的“守门员”，况且它已经暴露了如此多的问题。这也是为什么我推荐，资源用完即显式释放，或者利用资源池来尽量重用。 finalize还会掩盖资源回收时的出错信息，我们看下面一段JDK的源代码，截取自java.lang.ref.Finalizer private void runFinalizer(JavaLangAccess jla) { // ... 省略部分代码 try { Object finalizee = this.get(); if (finalizee != null &amp;&amp; !(finalizee insanceof java.lang.Enum)) { jla.invokeFinalize(finalizee); // Clear sack slot containing this variable, to decrease // the chances of false retention with a conservative GC finalizee = null; } } catch (Throwable x) { } super.clear(); } 结合我上期专栏介绍的异常处理实践，你认为这段代码会导致什么问题？是的，你没有看错，这里的Throwable是被生吞了的！也就意味着一旦出现异常或者出错，你得不到任何有效信息。况且，Java在finalize阶段也没有好的方式处理任何信息，不然更加不可预测。 3.有什么机制可以替换finalize吗？Java平台目前在逐步使用java.lang.ref.Cleaner来替换掉原有的finalize实现。Cleaner的实现利用了幻象引用（PhantomReference），这是一种常见的所谓post-mortem清理机制。我会在后面的专栏系统介绍Java的各种引用，利用幻象引用和引用队列，我们可以保证对象被彻底销毁前做一些类似资源回收的工作，比如关闭文件描述符（操作系统有限的资源），它比finalize更加轻量、更加可靠。 吸取了finalize里的教训，每个Cleaner的操作都是独立的，它有自己的运行线程，所以可以避免意外死锁等问题。 实践中，我们可以为自己的模块构建一个Cleaner，然后实现相应的清理逻辑。下面是JDK自身提供的样例程序： public class CleaningExample implements AutoCloseable { // A cleaner, preferably one shared within a library private satic final Cleaner cleaner = &lt;cleaner&gt;; satic class State implements Runnable { State(...) { // initialize State needed for cleaning action } public void run() { // cleanup action accessing State, executed at mos once } } private final State; private final Cleaner.Cleanable cleanable public CleaningExample() { this.sate = new State(...); this.cleanable = cleaner.regiser(this, sate); } public void close() { cleanable.clean(); } } 注意，从可预测性的角度来判断，Cleaner或者幻象引用改善的程度仍然是有限的，如果由于种种原因导致幻象引用堆积，同样会出现问题。所以，Cleaner适合作为一种最后的保证手段，而不是完全依赖Cleaner进行资源回收，不然我们就要再做一遍finalize的噩梦了。 我也注意到很多第三方库自己直接利用幻象引用定制资源收集，比如广泛使用的MySQL JDBC driver之一的mysql-connector-j，就利用了幻象引用机制。幻象引用也可以进行类似链条式依赖关系的动作，比如，进行总量控制的场景，保证只有连接被关闭，相应资源被回收，连接池才能创建新的连接。 另外，这种代码如果稍有不慎添加了对资源的强引用关系，就会导致循环引用关系，前面提到的MySQL JDBC就在特定模式下有这种问题，导致内存泄漏。上面的示例代码中，将State定义为static，就是为了避免普通的内部类隐含着对外部对象的强引用，因为那样会使外部对象无法进入幻象可达的状态。 今天，我从语法角度分析了final、finally、finalize，并从安全、性能、垃圾收集等方面逐步深入，探讨了实践中的注意事项，希望对你有所帮助。 一课一练关于今天我们讨论的题目你做到心中有数了吗？也许你已经注意到了，JDK自身使用的Cleaner机制仍然是有缺陷的，你有什么更好的建议吗？ 请你在留言区写写你的建议，我会选出经过认真思考的留言，送给你一份学习鼓励金，欢迎你与我一起讨论。你的朋友是不是也在准备面试呢？你可以“请朋友读”，把今天的题目分享给好友，或许你能帮到他。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础专题</category>
      </categories>
      <tags>
        <tag>阅读笔记</tag>
        <tag>Java核心36讲</tag>
        <tag>极客的专栏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02Exception和Error有什么区别[转]]]></title>
    <url>%2F2020%2F03%2F02%2FJava%E6%A0%B8%E5%BF%8336%E8%AE%B202%2F</url>
    <content type="text"><![CDATA[世界上存在永远不会出错的程序吗？也许这只会出现在程序员的梦中。随着编程语言和软件的诞生，异常情况就如影随形地纠缠着我们，只有正确处理好意外情况，才能保证程序的可靠性。 Java语言在设计之初就提供了相对完善的异常处理机制，这也是Java得以大行其道的原因之一，因为这种机制大大降低了编写和维护可靠程序的门槛。如今，异常处理机制已经成为现代编程语言的标配。 今天我要问你的问题是，请对比Exception和Error，另外，运行时异常与一般异常有什么区别？ 典型回答 Exception和Error都是继承了Throwable类，在Java中只有Throwable类型的实例才可以被抛出（throw）或者捕获（catch），它是异常处理机制的基本组成类型。 Exception和Error体现了Java平台设计者对不同异常情况的分类。 Exception是程序正常运行中，可以预料的意外情况，可能并且应该被捕获，进行相应处理。 Error是指在正常情况下，不大可能出现的情况，绝大部分的Error都会导致程序（比如JVM自身）处于非正常的、不可恢复状态。既然是非正常情况，所以不便于也不需要捕获，常见的比如OutOfMemoryError之类，都是Error的子类。 Exception又分为可检查（checked）异常和不检查（unchecked）异常， 可检查异常在源代码里必须显式地进行捕获处理，这是编译期检查的一部分。 前面我介绍的不可查的Error，是Throwable不是Exception。 不检查异常就是所谓的运行时异常，类似 NullPointerException、ArrayIndexOutOfBoundsException之类，通常是可以编码避免的逻辑错误，具体根据需要来判断是否需要捕获，并不会在编译期强制要求。 考点分析分析Exception和Error的区别，是从概念角度考察了Java处理机制。总的来说，还处于理解的层面，面试者只要阐述清楚就好了。 我们在日常编程中，如何处理好异常是比较考验功底的，我觉得需要掌握两个方面。 第一，理解Throwable、Exception、Error的设计和分类。比如，掌握那些应用最为广泛的子类，以及如何自定义异常等。 很多面试官会进一步追问一些细节，比如，你了解哪些Error、Exception或者RuntimeException？我画了一个简单的类图，并列出来典型例子，可以给你作为参考，至少做到基本心里有数。 其中有些子类型，最好重点理解一下，比如NoClassDefFoundError和ClassNotFoundException有什么区别，这也是个经典的入门题目。 第二，理解Java语言中操作Throwable的元素和实践。掌握最基本的语法是必须的，如try-catch-fnally块，throw、throws关键字等。与此同时，也要懂得如何处理典型场景。 异常处理代码比较繁琐，比如我们需要写很多千篇一律的捕获代码，或者在fnally里面做一些资源回收工作。随着Java语言的发展，引入了一些更加便利的特性，比如try-with￾resources和multiple catch，具体可以参考下面的代码段。在编译时期，会自动生成相应的处理逻辑，比如，自动按照约定俗成close那些扩展了AutoCloseable或者Closeable的对象。 try (BuferedReader br = new BuferedReader(…); BuferedWriter writer = new BuferedWriter(…)) {// Try-with-resources // do something catch ( IOException | XEception e) {// Multiple catch // Handle it } 知识扩展前面谈的大多是概念性的东西，下面我来谈些实践中的选择，我会结合一些代码用例进行分析。 先开看第一个吧，下面的代码反映了异常处理中哪些不当之处？ try { // 业务代码 // … Thread.sleep(1000L); } catch (Exception e) { // Ignore it } 这段代码虽然很短，但是已经违反了异常处理的两个基本原则。 第一，尽量不要捕获类似Exception这样的通用异常，而是应该捕获特定异常，在这里是Thread.sleep()抛出的InterruptedException。 这是因为在日常的开发和合作中，我们读代码的机会往往超过写代码，软件工程是门协作的艺术，所以我们有义务让自己的代码能够直观地体现出尽量多的信息，而泛泛的Exception之类，恰恰隐藏了我们的目的。另外，我们也要保证程序不会捕获到我们不希望捕获的异常。比如，你可能更希望RuntimeException被扩散出来，而不是被捕获。进一步讲，除非深思熟虑了，否则不要捕获Throwable或者Error，这样很难保证我们能够正确程序处理OutOfMemoryError。 第二，不要生吞（swallow）异常。这是异常处理中要特别注意的事情，因为很可能会导致非常难以诊断的诡异情况。 生吞异常，往往是基于假设这段代码可能不会发生，或者感觉忽略异常是无所谓的，但是千万不要在产品代码做这种假设！如果我们不把异常抛出来，或者也没有输出到日志（Logger）之类，程序可能在后续代码以不可控的方式结束。没人能够轻易判断究竟是哪里抛出了异常，以及是什么原因产生了异常。 再来看看第二段代码 try { // 业务代码 // … } catch (IOException e) { e.printStackTrace(); } 这段代码作为一段实验代码，它是没有任何问题的，但是在产品代码中，通常都不允许这样处理。你先思考一下这是为什么呢？ 我们先来看看printStackTrace()的文档，开头就是“Prints this throwable and its backtrace to the standard error stream”。问题就在这里，在稍微复杂一点的生产系统中，标准出错（STERR）不是个合适的输出选项，因为你很难判断出到底输出到哪里去了。 尤其是对于分布式系统，如果发生异常，但是无法找到堆栈轨迹（stacktrace），这纯属是为诊断设置障碍。所以，最好使用产品日志，详细地输出到日志系统里。 我们接下来看下面的代码段，体会一下Throw early, catch late原则。 public void readPreferences(String fleName){ //...perform operations... InputStream in = new FileInputStream(fleName); //...read the preferences fle... } 如果fleName是null，那么程序就会抛出NullPointerException，但是由于没有第一时间暴露出问题，堆栈信息可能非常令人费解，往往需要相对复杂的定位。这个NPE只是作为例子，实际产品代码中，可能是各种情况，比如获取配置失败之类的。在发现问题的时候，第一时间抛出，能够更加清晰地反映问题。 我们可以修改一下，让问题“throw early”，对应的异常信息就非常直观了。 public void readPreferences(String flename) { Objects. requireNonNull(flename); //...perform other operations... InputStream in = new FileInputStream(flename); //...read the preferences fle... } 至于“catch late”，其实是我们经常苦恼的问题，捕获异常后，需要怎么处理呢？最差的处理方式，就是我前面提到的“生吞异常”，本质上其实是掩盖问题。如果实在不知道如何处理，可以选择保留原有异常的cause信息，直接再抛出或者构建新的异常抛出去。在更高层面，因为有了清晰的（业务）逻辑，往往会更清楚合适的处理方式是什么。 有的时候，我们会根据需要自定义异常，这个时候除了保证提供足够的信息，还有两点需要考虑： 是否需要定义成Checked Exception，因为这种类型设计的初衷更是为了从异常情况恢复，作为异常设计者，我们往往有充足信息进行分类。 在保证诊断信息足够的同时，也要考虑避免包含敏感信息，因为那样可能导致潜在的安全问题。 如果我们看Java的标准类库，你可能注意到类似java.net.ConnectException，出错信息是类似“ Connection refused (Connection refused)”，而不包含具体的机器名、IP、端口等， 一个重要考量就是信息安全。类似的情况在日志中也有，比如，用户数据一般是不可以输出到日志里面的。 业界有一种争论（甚至可以算是某种程度的共识），Java语言的Checked Exception也许是个设计错误，反对者列举了几点：Checked Exception的假设是我们捕获了异常，然后恢复程序。但是，其实我们大多数情况下，根本就不可能恢复。Checked Exception的使用，已经大大偏离了最初的设计目的。Checked Exception不兼容functional编程，如果你写过Lambda/Stream代码，相信深有体会。 很多开源项目，已经采纳了这种实践，比如Spring、Hibernate等，甚至反映在新的编程语言设计中，比如Scala等。 如果有兴趣，你可以参考：http://literatejava.com/exceptions/checked-exceptions-javas-biggest-mistake/ 。 当然，很多人也觉得没有必要矫枉过正，因为确实有一些异常，比如和环境相关的IO、网络等，其实是存在可恢复性的，而且Java已经通过业界的海量实践，证明了其构建高质量软件的能力。我就不再进一步解读了，感兴趣的同学可以点击链接，观看Bruce Eckel在2018年全球软件开发大会QCon的分享Failing at Failing: How and Why We’ve Been Nonchalantly Moving Away From Exception Handling。 我们从性能角度来审视一下Java的异常处理机制，这里有两个可能会相对昂贵的地方： try-catch代码段会产生额外的性能开销，或者换个角度说，它往往会影响JVM对代码进行优化，所以建议仅捕获有必要的代码段，尽量不要一个大的try包住整段的代码；与此同时，利用异常控制代码流程，也不是一个好主意，远比我们通常意义上的条件语句（if/else、switch）要低效。 Java每实例化一个Exception，都会对当时的栈进行快照，这是一个相对比较重的操作。如果发生的非常频繁，这个开销可就不能被忽略了。 所以，对于部分追求极致性能的底层类库，有种方式是尝试创建不进行栈快照的Exception。这本身也存在争议，因为这样做的假设在于，我创建异常时知道未来是否需要堆栈。问题是，实际上可能吗？小范围或许可能，但是在大规模项目中，这么做可能不是个理智的选择。如果需要堆栈，但又没有收集这些信息，在复杂情况下，尤其是类似微服务这种分布式系统，这会大大增加诊断的难度。 当我们的服务出现反应变慢、吞吐量下降的时候，检查发生最频繁的Exception也是一种思路。关于诊断后台变慢的问题，我会在后面的Java性能基础模块中系统探讨。 今天，我 从一个常见的异常处理概念问题，简单总结了Java异常处理的机制。 并结合代码，分析了一些普遍认可的最佳实践，以及业界最新的一些异常使用共识。 最后，我分析了异常性能开销，希望对你有所帮助。 一课一练关于今天我们讨论的题目你做到心中有数了吗？可以思考一个问题，对于异常处理编程，不同的编程范式也会影响到异常处理策略，比如，现在非常火热的反应式编程（Reactive Stream），因为其本身是异步、基于事件机制的，所以出现异常情况，决不能简单抛出去；另外，由于代码堆栈不再是同步调用那种垂直的结构，这里的异常处理和日志需要更加小心，我们看到的往往是特定executor的堆栈，而不是业务方法调用关系。对于这种情况，你有什么好的办法吗？ 请你在留言区分享一下你的解决方案，我会选出经过认真思考的留言，送给你一份学习鼓励金，欢迎你与我一起讨论。 你的朋友是不是也在准备面试呢？你可以“请朋友读”，把今天的题目分享给好友，或许你能帮到他。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础专题</category>
      </categories>
      <tags>
        <tag>阅读笔记</tag>
        <tag>Java核心36讲</tag>
        <tag>极客的专栏</tag>
        <tag>异常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java三种编译方式]]></title>
    <url>%2F2020%2F03%2F02%2FJava%E4%B8%89%E7%A7%8D%E7%BC%96%E8%AF%91%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前端编译 JIT编译 AOT编译 前端编译把Java源码文件（.java）编译成Class文件(.class)的过程；也即把满足Java语言规范的程序转化为满足JVM规范所要求格式的功能； 优点： 这阶段的优化是指程序编码方面的； 许多Java语法新特性（”语法糖”：泛型、内部类等等），是靠前端编译器实现的,而不是依赖虚拟机； 编译成的Class文件可以直接给JVM解释器解释执行，省去编译时间，加快启动速度； 缺点： 对代码运行效率几乎没有任何优化措施； 解释执行效率较低，所以需要结合下面的JIT编译； 前端编译器：Oracle javac、Eclipse JDT中的增量式编译器（ECJ）等; 后端编译/即时(JIT)编译通过Java虚拟机（JVM）内置的即时编译器（Just In Time Compiler，JIT编译器）；在运行时把Class文件字节码编译成本地机器码的过程； 优点： 通过在运行时收集监控信息，把”热点代码”（Hot Spot Code）编译成与本地平台相关的机器码，并进行各种层次的优化； 可以大大提高执行效率； 缺点： 收集监控信息影响程序运行； 编译过程占用程序运行时间（如使得启动速度变慢）； 编译机器码占用内存； JIT编译器：HotSpot虚拟机的C1、C2编译器等； 另外，JIT编译速度及编译结果的优劣，是衡量一个JVM性能的很重要指标；所以对程序运行性能优化集中到这个阶段；也就是说可以对这个阶段进行JVM调优； 静态提前编译（Ahead Of Time，AOT编译）程序运行前，直接把Java源码文件（.java）编译成本地机器码的过程； 优点： 编译不占用运行时间，可以做一些较耗时的优化，并可加快程序启动； 把编译的本地机器码保存磁盘，不占用内存，并可多次使用； 缺点： 因为Java语言的动态性（如反射）带来了额外的复杂性，影响了静态编译代码的质量； 一般静态编译不如JIT编译的质量，这种方式用得比较少； 静态提前编译器（AOT编译器）：JAOTC、GCJ、Excelsior JET、ART (Android Runtime)等； 关于ART (Android Runtime)模式：ART虽然主要通过AOT编译支持Java的运行，但仍然带有解释器。 前端编译+JIT编译到这里，我们知道目前Java体系中主要还是采用前端编译+JIT编译的方式，如JDK中的HotSpot虚拟机。 前端编译+JIT编译方式的运作过程大体如下： 首先通过前端编译把符合Java语言规范的程序代码转化为满足JVM规范所要求Class格式； 然后程序启动时Class格式文件发挥作用，解释执行，省去编译时间，加快启动速度； 针对Class解释执行效率低的问题，在运行中收集性能监控信息，得知”热点代码”； JIT逐渐发挥作用，把越来越多的热点代码”编译优化成本地代码，提高执行效率；]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础知识点</category>
      </categories>
      <tags>
        <tag>阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01谈谈你对Java平台的理解?[转]]]></title>
    <url>%2F2020%2F03%2F01%2FJava%E6%A0%B8%E5%BF%8336%E8%AE%B201%2F</url>
    <content type="text"><![CDATA[从你接触Java开发到现在，你对Java最直观的印象是什么呢？是它宣传的 “Write once, run anywhere”，还是目前看已经有些过于形式主义的语法呢？你对于Java平台到底了解到什么程度？请你先停下来总结思考一下。 今天我要问你的问题是，谈谈你对Java平台的理解？“Java是解释执行”，这句话正确吗？ 典型回答Java本身是一种面向对象的语言，最显著的特性有两个方面， 一是所谓的“书写一次，到处运行”（Write once, run anywhere），能够非常容易地获得跨平台能力； 另外就是垃圾收集（GC, Garbage Collection），Java通过垃圾收集器（Garbage Collector）回收分配内存，大部分情况下，程序员不需要自己操心内存的分配和回收。 我们日常会接触到JRE（Java Runtime Environment）或者JDK（Java Development Kit）。 JRE，也就是Java运行环境，包含了JVM和Java类库，以及一些模块等。而JDK可以看作是JRE的一个超集，提供了更多工具，比如编译器、各种诊断工具等。 对于“Java是解释执行”这句话，这个说法不太准确。 我们开发的Java的源代码， 首先通过Javac编译成为字节码（bytecode），然后，在运行时，通过 Java虚拟机（JVM）内嵌的解释器将字节码转换成为最终的机器码。 但是常见的JVM，比如我们大多数情况使用的Oracle JDK提供的Hotspot JVM，都提供了JIT（Just-In-Time）编译器，也就是通常所说的动态编译器，JIT能够在# 运行时将热点代码编译成机器码 #，这种情况下部分热点代码就属于编译执行，而不是解释执行了。 考点分析其实这个问题，问得有点笼统。题目本身是非常开放的，往往考察的是多个方面，比如，基础知识理解是否很清楚；是否掌握Java平台主要模块和运行原理等。很多面试者会在这种问题上吃亏，稍微紧张了一下，不知道从何说起，就给出个很简略的回答。 对于这类笼统的问题，你需要尽量表现出自己的思维深入并系统化，Java知识理解得也比较全面，一定要避免让面试官觉得你是个“知其然不知其所以然”的人。毕竟明白基本组成和机制，是日常工作中进行问题诊断或者性能调优等很多事情的基础，相信没有招聘方会不喜欢“热爱学习和思考”的面试者。 即使感觉自己的回答不是非常完善，也不用担心。我个人觉得这种笼统的问题，有时候回答得稍微片面也很正常，大多数有经验的面试官，不会因为一道题就对面试者轻易地下结论。通常会尽量引导面试者，把他的真实水平展现出来，这种问题就是做个开场热身，面试官经常会根据你的回答扩展相关问题。 知识扩展回归正题，对于Java平台的理解，可以从很多方面简明扼要地谈一下，例如： Java语言特性，包括泛型、Lambda等语言特性； 基础类库，包括集合、IO/NIO、网络、并发、安全等基础类库。对于我们日常工作应用较多的类库，面试前可以系统化总结一下，有助于临场发挥。 或者谈谈JVM的一些基础概念和机制，比如 Java的类加载机制，常用版本JDK（如JDK 8）内嵌的Class-Loader，例如Bootstrap、 Application和Extension Class-loader； 类加载大致过程：加载、验证、链接、初始化（这里参考了周志明的《深入理解Java虚拟机》，非常棒的JVM上手书籍）； 自定义Class-Loader等。 还有垃圾收集的基本原理，最常见的垃圾收集器，如SerialGC、Parallel GC、 CMS、 G1等，对于适用于什么样的工作负载最好也心里有数。这些都是可以扩展开的领域，我会在后面的专栏对此进行更系统的介绍。 当然还有JDK包含哪些工具或者Java领域内其他工具等，如编译器、运行时环境、安全工具、诊断和监控工具等。这些基本工具是日常工作效率的保证，对于我们工作在其他语言平台上，同样有所帮助，很多都是触类旁通的。 下图是我总结的一个相对宽泛的蓝图供你参考。 不再扩展了，回到前面问到的解释执行和编译执行的问题。 有些面试官喜欢在特定问题上“刨根问底儿”，因为这是进一步了解面试者对知识掌握程度的有效方法，我稍微深入探讨一下。 众所周知，我们通常把Java分为编译期和运行时。 这里说的Java的编译和C/C++是有着不同的意义的，Javac的编译，编译Java源码生成“.class”文件里面实际是字节码，而不是可以直接执行的机器码。Java通过字节码和Java虚拟机（JVM）这种跨平台的抽象，屏蔽了操作系统和硬件的细节，这也是实现“一次编译，到处执行”的基础。 在运行时，JVM会通过类加载器（Class-Loader）加载字节码，解释或者编译执行。就像我前面提到的，主流Java版本中，如JDK 8实际是解释和编译混合的一种模式，即所谓的混合模式（-Xmixed）。通常运行在server模式的JVM，会进行上万次调用以收集足够的信息进行高效的编译，client模式这个门限是1500次。Oracle Hotspot JVM内置了两个不同的JIT compiler，C1对应前面说的client模式，适用于对于启动速度敏感的应用，比如普通Java桌面应用；C2对应server模式，它的优化是为长时间运行的服务器端应用设计的。默认是采用所谓的分层编译（TieredCompilation）。这里不再展开更多JIT的细节，没必要一下子就钻进去，我会在后面介绍分层编译的内容。 Java虚拟机启动时，可以指定不同的参数对运行模式进行选择。 比如， 指定“-Xint”，就是告诉JVM只进行解释执行，不对代码进行编译，这种模式抛弃了JIT可能带来的性能优势。毕竟解释器（interpreter）是逐条读入，逐条解释运行的。 与其相对应的，还有一个“-Xcomp”参数，这是告诉JVM关闭解释器，不要进行解释执行，或者叫作最大优化级别。那你可能会问这种模式是不是最高效啊？简单说，还真未必。“-Xcomp”会导致JVM启动变慢非常多，同时有些JIT编译器优化方式，比如分支预测，如果不进行profling，往往并不能进行有效优化。 除了我们日常最常见的Java使用模式，其实还有一种新的编译方式，即所谓的AOT（Ahead-of-Time Compilation），直接将字节码编译成机器代码，这样就避免了JIT预热等各方面的开销，比如Oracle JDK 9就引入了实验性的AOT特性，并且增加了新的jaotc工具。利用下面的命令把某个类或者某个模块编译成为AOT库。 jaotc --output libHelloWorld.so HelloWorld.class jaotc --output libjava.base.so --module java.base 然后，在启动时直接指定就可以了。 java -XX:AOTLibrary=./libHelloWorld.so,./libjava.base.so HelloWorld 而且，Oracle JDK支持分层编译和AOT协作使用，这两者并不是二选一的关系。如果你有兴趣，可以参考相关文档：http://openjdk.java.net/jeps/295 。AOT也不仅仅是只有这一种方式，业界早就有第三方工具（如GCJ、Excelsior JET）提供相关功能。 另外，JVM作为一个强大的平台，不仅仅只有Java语言可以运行在JVM上，本质上合规的字节码都可以运行，Java语言自身也为此提供了便利，我们可以看到类似Clojure、Scala、Groovy、JRuby、Jython等大量JVM语言，活跃在不同的场景。 今天，我简单介绍了一下Java平台相关的一些内容，目的是提纲挈领地构建一个整体的印象，包括Java语言特性、 核心类库与常用第三方类库、Java虚拟机基本原理和相关工具，希望对你有所帮助。 一课一练关于今天我们讨论的题目你做到心中有数了吗？知道不如做到，请你也在留言区写写自己对Java平台的理解。我会选出经过认真思考的留言，送给你一份学习鼓励金，欢迎你与我一起讨论。你的朋友是不是也在准备面试呢？你可以“请朋友读”，把今天的题目分享给好友，或许你能帮到他。]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础专题</category>
      </categories>
      <tags>
        <tag>阅读笔记</tag>
        <tag>Java核心36讲</tag>
        <tag>极客的专栏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中的位运算^,&,,[转]]]></title>
    <url>%2F2020%2F02%2F29%2Fjava%E4%B8%AD%E7%9A%84%E4%BD%8D%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[^(亦或运算) ，针对二进制，相同的为0，不同的为1 123456789public static void main(String[] args) &#123; System.out.println(&quot;2^3运算的结果是 :&quot;+(2^3)); //打印的结果是: 2^3运算的结果是 :1&#125;2 =======&gt;00103 =======&gt;00112^3就为0001，结果就是1 &amp;（与运算） 针对二进制，只要有一个为0，就为0还是上述的例子 1234public static void main(String[] args) &#123; System.out.println(&quot;2&amp;3运算的结果是 :&quot;+(2&amp;3)); //打印的结果是: 2&amp;3运算的结果是 :2 &#125; &lt;&lt;(向左位移) 针对二进制，转换成二进制后向左移动3位，后面用0补齐1234public static void main(String[] args) &#123; System.out.println(&quot;2&lt;&lt;3运算的结果是 :&quot;+(2&lt;&lt;3)); //打印的结果是: 2&lt;&lt;3运算的结果是 :16 &#125; &gt;&gt;(向右位移) 针对二进制，转换成二进制后向右移动3位，1234public static void main(String[] args) &#123; System.out.println(&quot;2&gt;&gt;3运算的结果是 :&quot;+(2&gt;&gt;3)); //打印的结果是: 2&gt;&gt;3运算的结果是 :0 &#125; &gt;&gt;&gt;(无符号右移) 无符号右移，忽略符号位，空位都以0补齐10进制转二进制的时候，因为二进制数一般分8位、 16位、32位以及64位 表示一个十进制数，所以在转换过程中，最高位会补零。 在计算机中负数采用二进制的补码表示，10进制转为二进制得到的是源码，将源码按位取反得到的是反码，反码加1得到补码 二进制的最高位是符号位，0表示正，1表示负。 &gt;&gt;&gt;与&gt;&gt;唯一的不同是它无论原来的最左边是什么数，统统都用0填充。——比如，byte是8位的，-1表示为byte型是11111111(补码表示法）b&gt;&gt;&gt;4就是无符号右移4位，即00001111，这样结果就是15。下面看代码 123456789101112131415161718public static void main(String[] args) &#123; System.out.println(&quot;16&gt;&gt;2运算的结果是 :&quot;+((16)&gt;&gt;2)); //打印的结果是: 16&gt;&gt;2运算的结果是 :4&#125;public static void main(String[] args) &#123; System.out.println(&quot;-16&gt;&gt;2运算的结果是 :&quot;+((-16)&gt;&gt;2)); //打印的结果是: -16&gt;&gt;2运算的结果是 :-4 &#125;public static void main(String[] args) &#123; System.out.println(&quot;16&gt;&gt;&gt;2运算的结果是 :&quot;+((16)&gt;&gt;&gt;2)); //打印的结果是: 16&gt;&gt;&gt;2运算的结果是 :4 &#125;public static void main(String[] args) &#123; System.out.println(&quot;-16&gt;&gt;&gt;2运算的结果是 :&quot;+((-16)&gt;&gt;&gt;2)); //打印的结果是: -16&gt;&gt;&gt;2运算的结果是 :1073741820 &#125; 可见正数做&gt;&gt;&gt;运算的时候和&gt;&gt;是一样的。区别在于负数运算]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础知识点</category>
      </categories>
      <tags>
        <tag>阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[helloworld]]></title>
    <url>%2F2019%2F05%2F23%2Fhelloworld%2F</url>
    <content type="text"></content>
  </entry>
</search>
